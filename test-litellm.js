import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import OpenAI from 'openai';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

dotenv.config({ path: path.join(__dirname, '.env') });

async function testLiteLLM() {
  console.log('ğŸ§ª Testing LiteLLM Configuration...\n');
  
  const baseURL = process.env.LITELLM_BASE_URL;
  const apiKey = process.env.LITELLM_API_KEY;
  
  console.log('ğŸ“‹ Configuration:');
  console.log(`   Base URL: ${baseURL}`);
  console.log(`   API Key: ${apiKey ? 'âœ“ Set' : 'âœ— Missing'}\n`);
  
  if (!baseURL || !apiKey) {
    console.error('âŒ Error: LITELLM_BASE_URL or LITELLM_API_KEY not configured!');
    process.exit(1);
  }
  
  try {
    console.log('ğŸ”— Connecting to LiteLLM...');
    
    const client = new OpenAI({
      baseURL: baseURL,
      apiKey: apiKey,
    });
    
    console.log('âœ“ Client initialized\n');
    
    console.log('ğŸ“¤ Sending test request to LiteLLM...');
    const response = await client.chat.completions.create({
      model: 'gemini/gemini-2.5-flash',
      messages: [
        {
          role: 'user',
          content: 'Hello! Just a quick test. Reply with one word.',
        },
      ],
      temperature: 0.7,
      top_p: 0.9,
    });
    
    console.log('âœ… Success! Response received:\n');
    console.log('ğŸ“ Model:', response.model);
    console.log('ğŸ’¬ Message:', response.choices[0].message.content);
    console.log('âš¡ Tokens used:', response.usage.total_tokens);
    console.log('\nâœ… LiteLLM is working correctly!');
    
    process.exit(0);
  } catch (error) {
    console.error('\nâŒ Error testing LiteLLM:');
    console.error('   Error:', error.message);
    
    if (error.response) {
      console.error('   Status:', error.response.status);
      console.error('   Data:', error.response.data);
    }
    
    process.exit(1);
  }
}

testLiteLLM();
